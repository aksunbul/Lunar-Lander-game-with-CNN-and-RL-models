{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander Control- Assignment 2\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment aims to train an agent to play the Lunar Lander game using a convolutional neural network and a reinforcement learning algorithm.\n",
    "\n",
    "In the OpenAI Gym game Lunar Lander (https://gym.openai.com/envs/LunarLander-v2/) the player's job is to control a small spaceship to land if safely on a landing pad. There are three thrusters which can be used for control. These work in three directions: up, left, and right. The player can also choose to do nothing. A dataset has been collected from an expert player of LunarLander that contains screenshots of the state of the game and the player's associated action (none, up, left, and right).\n",
    "\n",
    "Training each of the models requires the following steps:\n",
    "1. Import and preprocess data\n",
    "    - Images are converted to grayscale, downsized to 84x84, and each pixel is normalised by dividing by 255\n",
    "2. Train model using 70 percent of the original data set\n",
    "    - Validation set is 15 percent and test set is 15 percent\n",
    "3. Evalutation exercise as game controller for 200 episodes\n",
    "    - Using a lunar lander game player for each model:\n",
    "         Convolutional Neural Network: lunar_lander_ml_images_player.py\n",
    "         Deep Q-Learning with Epsilon Greedy Policy: lunar_lander_rl_player.py\n",
    "4. The models above are then evaluated based on their performance in the Lunar Lander game. They are compared based on computation time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages to use in model building and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksunbul\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from tensorflow.python.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import RMSprop, adam\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import sklearn\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "import rl\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import PIL\n",
    "from IPython.display import display, HTML, Image, SVG\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import cv2\n",
    "import gym\n",
    "import os, os.path, shutil\n",
    "\n",
    "# import shutil\n",
    "# from IPython.display import SVG\n",
    "# import cv2\n",
    "# import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split images into class action folders in all_data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folder path\n",
    "folder_path=('\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\all_data')\n",
    "\n",
    "images = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "for image in images:\n",
    "    frame, date, number, folder_name = image.split('.')[0].split('_')\n",
    "    # eg. frame_2019-04-08-11-20-48_8_0.jpeg\n",
    "\n",
    "    new_path = os.path.join(folder_path, folder_name)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "\n",
    "    old_image_path = os.path.join(folder_path, image)\n",
    "    new_image_path = os.path.join(new_path, image)\n",
    "    shutil.move(old_image_path, new_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split folders into training and validation folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: To use the split dataset function, we must have subdirectories named 'all_data', 'training_data', and 'validation_data' in the main directory as the notebook file. The all_data directory must split all images into their own subdirectories by category. It will randomly select training and validation set and create subdirectories by category in 'training_data' and 'validation_data' subdirectories. We use import the directories into generators for trainin models in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was created by Daan Raman, which was shared on Github. It can be found at https://github.com/keras-team/keras/issues/5862.\n",
    "\n",
    "def split_dataset_into_test_and_train_sets(all_data_dir, training_data_dir, testing_data_dir, testing_data_pct):\n",
    "    # Recreate testing and training directories\n",
    "    if testing_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(testing_data_dir, ignore_errors=False)\n",
    "        os.makedirs(testing_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + testing_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + testing_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    if training_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(training_data_dir, ignore_errors=False)\n",
    "        os.makedirs(training_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + training_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete training data directory \" + training_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    num_training_files = 0\n",
    "    num_testing_files = 0\n",
    "\n",
    "    for subdir, dirs, files in os.walk(all_data_dir):\n",
    "        category_name = os.path.basename(subdir)\n",
    "\n",
    "        # Don't create a subdirectory for the root directory\n",
    "        print(category_name + \" vs \" + os.path.basename(all_data_dir))\n",
    "        if category_name == os.path.basename(all_data_dir):\n",
    "            continue\n",
    "\n",
    "        training_data_category_dir = training_data_dir + '/' + category_name\n",
    "        testing_data_category_dir = testing_data_dir + '/' + category_name\n",
    "\n",
    "        if not os.path.exists(training_data_category_dir):\n",
    "            os.mkdir(training_data_category_dir)\n",
    "\n",
    "        if not os.path.exists(testing_data_category_dir):\n",
    "            os.mkdir(testing_data_category_dir)\n",
    "\n",
    "        for file in files:\n",
    "            input_file = os.path.join(subdir, file)\n",
    "            if np.random.rand(1) < testing_data_pct:\n",
    "                shutil.copy(input_file, testing_data_dir + '/' + category_name + '/' + file)\n",
    "                num_testing_files += 1\n",
    "            else:\n",
    "                shutil.copy(input_file, training_data_dir + '/' + category_name + '/' + file)\n",
    "                num_training_files += 1\n",
    "\n",
    "    print(\"Processed \" + str(num_training_files) + \" training files.\")\n",
    "    print(\"Processed \" + str(num_testing_files) + \" testing files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refusing to delete testing data directory validation_data as we prevent you from doing stupid things!\n",
      "Refusing to delete training data directory training_data as we prevent you from doing stupid things!\n",
      "all_data vs all_data\n",
      "0 vs all_data\n",
      "1 vs all_data\n",
      "2 vs all_data\n",
      "3 vs all_data\n",
      "Processed 44637 training files.\n",
      "Processed 19034 testing files.\n"
     ]
    }
   ],
   "source": [
    "split_dataset_into_test_and_train_sets('all_data', 'training_data', 'validation_data', 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance classes of actions in TRAINING data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only training data will be balanced according to the samllest class. Class folder 1 has approx 1500 images, therefore we will try to reduce the other folder sizes to withing 2 times the size of this, using an undersampling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2788, 1577, 3211, 1525])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "dest0 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\0\\\\*.jpeg'\n",
    "dest1 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\1\\\\*.jpeg'\n",
    "dest2 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\2\\\\*.jpeg'\n",
    "dest3 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\3\\\\*.jpeg'\n",
    "# dest1 = '/Users/kate/Documents/UCD_2018_2019/Semester2/AdvancedMachineLearningCOMP47590/Assignment_2/balanced_train_data/*.jpeg'\n",
    "\n",
    "file_list0 = glob.glob(dest0)\n",
    "file_list1 = glob.glob(dest1)\n",
    "file_list2 = glob.glob(dest2)\n",
    "file_list3 = glob.glob(dest3)\n",
    "\n",
    "import numpy\n",
    "len_arr = numpy.array([len(file_list0),len(file_list1),len(file_list2),len(file_list3)])\n",
    "len_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data folder sizes:\n",
    "- 0: 21168\n",
    "- 1: 1577\n",
    "- 2: 20207\n",
    "- 3: 1525\n",
    "\n",
    "We want to reduce folders 0 and 2 to max 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take images out of folders in order to balance classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "# Downsize images in folder '0'\n",
    "dest0 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\0\\\\*.jpeg'\n",
    "# dest0 = '/Users/kate/Documents/UCD_2018_2019/Semester2/AdvancedMachineLearningCOMP47590/Assignment_2/training_data/0/*.jpeg'\n",
    "file_list = glob.glob(dest0)\n",
    "\n",
    "random.shuffle(file_list)\n",
    "new_images0 = file_list[0:18268]\n",
    "\n",
    "for file in new_images0:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of images in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2788,  1577, 20479,  1525])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding number of images in each folder\n",
    "import glob\n",
    "import random\n",
    "dest0 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\0\\\\*.jpeg'\n",
    "dest1 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\1\\\\*.jpeg'\n",
    "dest2 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\2\\\\*.jpeg'\n",
    "dest3 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\3\\\\*.jpeg'\n",
    "# dest1 = '/Users/kate/Documents/UCD_2018_2019/Semester2/AdvancedMachineLearningCOMP47590/Assignment_2/balanced_train_data/*.jpeg'\n",
    "\n",
    "file_list0 = glob.glob(dest0)\n",
    "file_list1 = glob.glob(dest1)\n",
    "file_list2 = glob.glob(dest2)\n",
    "file_list3 = glob.glob(dest3)\n",
    "\n",
    "import numpy\n",
    "len_arr = numpy.array([len(file_list0),len(file_list1),len(file_list2),len(file_list3)])\n",
    "len_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsize images in folder '2'\n",
    "dest2 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\2\\\\*.jpeg'\n",
    "# dest2 = '/Users/kate/Documents/UCD_2018_2019/Semester2/AdvancedMachineLearningCOMP47590/Assignment_2/training_data/2/*.jpeg'\n",
    "file_list = glob.glob(dest2)\n",
    "\n",
    "random.shuffle(file_list)\n",
    "new_images2 = file_list[0:17268]\n",
    "\n",
    "for file in new_images2:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of images in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2788, 1577, 3211, 1525])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding number of images in each folder\n",
    "import glob\n",
    "import random\n",
    "dest0 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\0\\\\*.jpeg'\n",
    "dest1 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\1\\\\*.jpeg'\n",
    "dest2 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\2\\\\*.jpeg'\n",
    "dest3 = '\\\\Users\\\\aksunbul\\\\Desktop\\\\MSc Statistics\\\\Advanced Machine Learning\\\\Assignments\\\\Assignment 2\\\\assignment\\\\training_data\\\\3\\\\*.jpeg'\n",
    "# dest1 = '/Users/kate/Documents/UCD_2018_2019/Semester2/AdvancedMachineLearningCOMP47590/Assignment_2/balanced_train_data/*.jpeg'\n",
    "\n",
    "file_list0 = glob.glob(dest0)\n",
    "file_list1 = glob.glob(dest1)\n",
    "file_list2 = glob.glob(dest2)\n",
    "file_list3 = glob.glob(dest3)\n",
    "\n",
    "import numpy\n",
    "len_arr = numpy.array([len(file_list0),len(file_list1),len(file_list2),len(file_list3)])\n",
    "len_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsize and change images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9101 images belonging to 4 classes.\n",
      "Found 19034 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\n",
    "# Create generators for training and test data and rescale image pixels by 255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Convert images to grayscale and resize to dimension 64x64\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'training_data',\n",
    "        target_size=(84, 84),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        color_mode = 'grayscale')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'validation_data',\n",
    "        target_size=(84, 84),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        color_mode = 'grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Convolutional Neural Network Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 82, 82, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 41, 41, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 39, 39, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 17, 17, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 2,155,460\n",
      "Trainable params: 2,155,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build a CNN\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding = 'valid', activation = 'relu', input_shape = (84, 84, 1)))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Conv2D(64, (3,3), padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Conv2D(64, (3,3), padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on the training data and test on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "284/284 [==============================] - 1516s 5s/step - loss: 1.0433 - acc: 0.5454 - val_loss: 0.7793 - val_acc: 0.6965\n",
      "Epoch 2/4\n",
      "284/284 [==============================] - 1138s 4s/step - loss: 0.8829 - acc: 0.6186 - val_loss: 0.7482 - val_acc: 0.6942\n",
      "Epoch 3/4\n",
      "284/284 [==============================] - 915s 3s/step - loss: 0.8238 - acc: 0.6416 - val_loss: 0.7518 - val_acc: 0.6780\n",
      "Epoch 4/4\n",
      "284/284 [==============================] - 929s 3s/step - loss: 0.7791 - acc: 0.6595 - val_loss: 0.7618 - val_acc: 0.6710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x149b3f812b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//validation_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ML_image_model_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 80, 80, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 40, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 38, 38, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 2,191,620\n",
      "Trainable params: 2,191,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build a CNN\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (5,5), padding = 'valid', activation = 'relu', input_shape = (84, 84, 1)))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Conv2D(64, (3,3), padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Conv2D(128, (3,3), padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "284/284 [==============================] - 935s 3s/step - loss: 1.0522 - acc: 0.5454 - val_loss: 0.8102 - val_acc: 0.6674\n",
      "Epoch 2/4\n",
      "284/284 [==============================] - 656s 2s/step - loss: 0.9066 - acc: 0.6152 - val_loss: 0.7594 - val_acc: 0.7003\n",
      "Epoch 3/4\n",
      "284/284 [==============================] - 617s 2s/step - loss: 0.8436 - acc: 0.6365 - val_loss: 0.7635 - val_acc: 0.6526\n",
      "Epoch 4/4\n",
      "284/284 [==============================] - 592s 2s/step - loss: 0.7983 - acc: 0.6534 - val_loss: 0.7391 - val_acc: 0.6878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f7e61eccc0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//validation_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ML_image_model_v2_newParameters.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Reinforcement Learning with Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'LunarLander-v2'\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 756\n",
      "Trainable params: 756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Deep Q Learning with Epsilon Greedy policy\n",
    "memory = SequentialMemory(limit=300000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=200,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "    80/100000: episode: 1, duration: 0.136s, episode steps: 80, steps per second: 588, episode reward: -611.151, mean reward: -7.639 [-100.000, 2.843], mean action: 1.550 [0.000, 2.000], mean observation: 0.136 [-5.029, 3.282], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   134/100000: episode: 2, duration: 0.037s, episode steps: 54, steps per second: 1442, episode reward: -479.214, mean reward: -8.874 [-100.000, -1.844], mean action: 1.407 [0.000, 2.000], mean observation: 0.080 [-2.033, 7.817], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   220/100000: episode: 3, duration: 0.810s, episode steps: 86, steps per second: 106, episode reward: -732.691, mean reward: -8.520 [-100.000, -0.404], mean action: 1.349 [0.000, 3.000], mean observation: 0.235 [-1.891, 4.834], loss: 63.888624, mean_absolute_error: 1.949395, mean_q: 0.291975\n",
      "   306/100000: episode: 4, duration: 0.277s, episode steps: 86, steps per second: 310, episode reward: -133.441, mean reward: -1.552 [-100.000, 7.250], mean action: 0.326 [0.000, 3.000], mean observation: -0.053 [-1.858, 5.669], loss: 60.227356, mean_absolute_error: 1.835983, mean_q: 0.232982\n",
      "   396/100000: episode: 5, duration: 0.279s, episode steps: 90, steps per second: 323, episode reward: -114.739, mean reward: -1.275 [-100.000, 7.488], mean action: 0.244 [0.000, 3.000], mean observation: 0.086 [-1.844, 6.067], loss: 44.329304, mean_absolute_error: 2.203250, mean_q: 0.339574\n",
      "   462/100000: episode: 6, duration: 0.197s, episode steps: 66, steps per second: 335, episode reward: -110.344, mean reward: -1.672 [-100.000, 18.922], mean action: 0.758 [0.000, 3.000], mean observation: -0.020 [-1.759, 1.405], loss: 54.851406, mean_absolute_error: 2.683011, mean_q: -0.411780\n",
      "   524/100000: episode: 7, duration: 0.202s, episode steps: 62, steps per second: 307, episode reward: -541.782, mean reward: -8.738 [-100.000, -2.032], mean action: 2.597 [1.000, 3.000], mean observation: -0.252 [-3.748, 2.840], loss: 58.668358, mean_absolute_error: 3.108932, mean_q: -0.848203\n",
      "   585/100000: episode: 8, duration: 0.184s, episode steps: 61, steps per second: 332, episode reward: -124.588, mean reward: -2.042 [-100.000, 7.532], mean action: 0.082 [0.000, 3.000], mean observation: -0.047 [-6.373, 1.400], loss: 62.689758, mean_absolute_error: 3.196095, mean_q: -1.136603\n",
      "   658/100000: episode: 9, duration: 0.236s, episode steps: 73, steps per second: 309, episode reward: -166.556, mean reward: -2.282 [-100.000, 6.232], mean action: 0.082 [0.000, 3.000], mean observation: -0.061 [-6.023, 1.412], loss: 49.661907, mean_absolute_error: 3.234628, mean_q: -1.749618\n",
      "   743/100000: episode: 10, duration: 0.260s, episode steps: 85, steps per second: 326, episode reward: -105.716, mean reward: -1.244 [-100.000, 12.902], mean action: 0.094 [0.000, 3.000], mean observation: 0.055 [-1.774, 1.471], loss: 47.321148, mean_absolute_error: 4.159891, mean_q: -3.076836\n",
      "   828/100000: episode: 11, duration: 0.259s, episode steps: 85, steps per second: 328, episode reward: -105.944, mean reward: -1.246 [-100.000, 8.889], mean action: 0.106 [0.000, 3.000], mean observation: 0.041 [-5.563, 1.461], loss: 43.972664, mean_absolute_error: 5.118528, mean_q: -4.186112\n",
      "   913/100000: episode: 12, duration: 0.296s, episode steps: 85, steps per second: 287, episode reward: -162.952, mean reward: -1.917 [-100.000, 9.057], mean action: 0.129 [0.000, 3.000], mean observation: 0.094 [-1.883, 1.426], loss: 54.579159, mean_absolute_error: 5.818262, mean_q: -5.301255\n",
      "  1006/100000: episode: 13, duration: 0.341s, episode steps: 93, steps per second: 272, episode reward: -610.085, mean reward: -6.560 [-100.000, 0.000], mean action: 1.839 [0.000, 3.000], mean observation: 0.490 [-1.019, 3.395], loss: 47.227806, mean_absolute_error: 7.440494, mean_q: -7.763147\n",
      "  1080/100000: episode: 14, duration: 0.238s, episode steps: 74, steps per second: 311, episode reward: -311.791, mean reward: -4.213 [-100.000, 1.646], mean action: 0.541 [0.000, 3.000], mean observation: 0.254 [-1.863, 1.958], loss: 43.516529, mean_absolute_error: 8.530795, mean_q: -9.283099\n",
      "  1154/100000: episode: 15, duration: 0.239s, episode steps: 74, steps per second: 309, episode reward: -293.366, mean reward: -3.964 [-100.000, 4.804], mean action: 0.297 [0.000, 3.000], mean observation: 0.084 [-1.795, 4.450], loss: 39.534271, mean_absolute_error: 10.198853, mean_q: -11.395571\n",
      "  1212/100000: episode: 16, duration: 0.183s, episode steps: 58, steps per second: 318, episode reward: -131.548, mean reward: -2.268 [-100.000, 13.470], mean action: 1.069 [0.000, 3.000], mean observation: -0.069 [-6.020, 1.394], loss: 20.255873, mean_absolute_error: 10.896905, mean_q: -12.584633\n",
      "  1284/100000: episode: 17, duration: 0.202s, episode steps: 72, steps per second: 357, episode reward: -329.623, mean reward: -4.578 [-100.000, 1.412], mean action: 0.347 [0.000, 3.000], mean observation: 0.155 [-1.834, 1.997], loss: 23.092583, mean_absolute_error: 12.341143, mean_q: -14.186799\n",
      "  1367/100000: episode: 18, duration: 0.207s, episode steps: 83, steps per second: 402, episode reward: -518.047, mean reward: -6.242 [-100.000, 1.607], mean action: 0.458 [0.000, 3.000], mean observation: 0.234 [-1.937, 3.575], loss: 12.552504, mean_absolute_error: 14.116843, mean_q: -16.381756\n",
      "  1454/100000: episode: 19, duration: 0.226s, episode steps: 87, steps per second: 385, episode reward: -472.757, mean reward: -5.434 [-100.000, 0.609], mean action: 0.310 [0.000, 3.000], mean observation: 0.258 [-5.377, 3.407], loss: 10.397066, mean_absolute_error: 15.699222, mean_q: -18.555502\n",
      "  1532/100000: episode: 20, duration: 0.209s, episode steps: 78, steps per second: 372, episode reward: -391.528, mean reward: -5.020 [-100.000, 7.260], mean action: 0.731 [0.000, 3.000], mean observation: 0.107 [-1.698, 4.015], loss: 8.747502, mean_absolute_error: 17.519329, mean_q: -21.339581\n",
      "  1612/100000: episode: 21, duration: 0.213s, episode steps: 80, steps per second: 376, episode reward: -534.519, mean reward: -6.681 [-100.000, 1.899], mean action: 0.525 [0.000, 2.000], mean observation: 0.421 [-1.858, 4.403], loss: 10.500750, mean_absolute_error: 19.873518, mean_q: -24.345280\n",
      "  1671/100000: episode: 22, duration: 0.164s, episode steps: 59, steps per second: 359, episode reward: -119.324, mean reward: -2.022 [-100.000, 6.191], mean action: 0.559 [0.000, 3.000], mean observation: -0.049 [-6.100, 1.396], loss: 6.859693, mean_absolute_error: 22.325914, mean_q: -27.520462\n",
      "  1754/100000: episode: 23, duration: 0.208s, episode steps: 83, steps per second: 399, episode reward: -225.680, mean reward: -2.719 [-100.000, 31.891], mean action: 0.904 [0.000, 3.000], mean observation: 0.183 [-1.755, 4.731], loss: 8.188289, mean_absolute_error: 23.051554, mean_q: -28.714432\n",
      "  1824/100000: episode: 24, duration: 0.187s, episode steps: 70, steps per second: 375, episode reward: -126.965, mean reward: -1.814 [-100.000, 35.023], mean action: 1.257 [0.000, 3.000], mean observation: -0.023 [-4.092, 1.414], loss: 8.037239, mean_absolute_error: 24.027109, mean_q: -29.800772\n",
      "  1919/100000: episode: 25, duration: 0.248s, episode steps: 95, steps per second: 384, episode reward: -176.341, mean reward: -1.856 [-100.000, 8.000], mean action: 0.653 [0.000, 3.000], mean observation: 0.213 [-5.928, 1.502], loss: 14.004210, mean_absolute_error: 25.351194, mean_q: -31.039743\n",
      "  2007/100000: episode: 26, duration: 0.246s, episode steps: 88, steps per second: 358, episode reward: -198.216, mean reward: -2.252 [-100.000, 16.883], mean action: 1.386 [0.000, 3.000], mean observation: 0.225 [-1.703, 4.660], loss: 10.025425, mean_absolute_error: 26.668755, mean_q: -32.797226\n",
      "  2059/100000: episode: 27, duration: 0.131s, episode steps: 52, steps per second: 398, episode reward: -110.731, mean reward: -2.129 [-100.000, 8.028], mean action: 0.577 [0.000, 3.000], mean observation: -0.106 [-5.690, 1.387], loss: 9.549953, mean_absolute_error: 26.936687, mean_q: -33.447334\n",
      "  2144/100000: episode: 28, duration: 0.220s, episode steps: 85, steps per second: 386, episode reward: -103.563, mean reward: -1.218 [-100.000, 7.995], mean action: 0.529 [0.000, 3.000], mean observation: 0.044 [-1.787, 6.120], loss: 8.740931, mean_absolute_error: 27.315136, mean_q: -33.512806\n",
      "  2209/100000: episode: 29, duration: 0.161s, episode steps: 65, steps per second: 405, episode reward: -141.850, mean reward: -2.182 [-100.000, 9.276], mean action: 0.908 [0.000, 3.000], mean observation: -0.061 [-1.736, 5.460], loss: 9.017782, mean_absolute_error: 27.626247, mean_q: -34.068714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2285/100000: episode: 30, duration: 0.200s, episode steps: 76, steps per second: 380, episode reward: -102.128, mean reward: -1.344 [-100.000, 10.619], mean action: 0.724 [0.000, 3.000], mean observation: -0.028 [-1.582, 4.637], loss: 13.179567, mean_absolute_error: 28.402786, mean_q: -35.223862\n",
      "  2359/100000: episode: 31, duration: 0.181s, episode steps: 74, steps per second: 409, episode reward: -74.615, mean reward: -1.008 [-100.000, 19.660], mean action: 0.500 [0.000, 3.000], mean observation: -0.029 [-1.568, 1.413], loss: 9.157174, mean_absolute_error: 28.363958, mean_q: -35.429417\n",
      "  2424/100000: episode: 32, duration: 0.173s, episode steps: 65, steps per second: 375, episode reward: -91.888, mean reward: -1.414 [-100.000, 13.965], mean action: 0.477 [0.000, 3.000], mean observation: -0.086 [-5.292, 1.408], loss: 8.510504, mean_absolute_error: 29.380829, mean_q: -36.841072\n",
      "  2482/100000: episode: 33, duration: 0.142s, episode steps: 58, steps per second: 407, episode reward: -112.810, mean reward: -1.945 [-100.000, 7.709], mean action: 0.190 [0.000, 3.000], mean observation: 0.003 [-1.820, 1.393], loss: 10.947007, mean_absolute_error: 31.350697, mean_q: -39.015408\n",
      "  2549/100000: episode: 34, duration: 0.178s, episode steps: 67, steps per second: 376, episode reward: -103.817, mean reward: -1.550 [-100.000, 5.810], mean action: 1.060 [0.000, 3.000], mean observation: -0.104 [-1.408, 4.149], loss: 8.486308, mean_absolute_error: 29.896437, mean_q: -37.284298\n",
      "  2608/100000: episode: 35, duration: 0.148s, episode steps: 59, steps per second: 398, episode reward: -110.065, mean reward: -1.866 [-100.000, 6.765], mean action: 0.898 [0.000, 3.000], mean observation: 0.007 [-1.761, 1.393], loss: 15.121803, mean_absolute_error: 30.768024, mean_q: -38.281303\n",
      "  2690/100000: episode: 36, duration: 0.216s, episode steps: 82, steps per second: 380, episode reward: -31.122, mean reward: -0.380 [-100.000, 95.338], mean action: 0.780 [0.000, 3.000], mean observation: 0.142 [-1.955, 1.416], loss: 8.592061, mean_absolute_error: 31.610256, mean_q: -39.903404\n",
      "  2760/100000: episode: 37, duration: 0.181s, episode steps: 70, steps per second: 386, episode reward: -1.942, mean reward: -0.028 [-100.000, 17.097], mean action: 1.471 [0.000, 3.000], mean observation: -0.081 [-1.341, 1.996], loss: 20.524649, mean_absolute_error: 32.313732, mean_q: -40.606861\n",
      "  2835/100000: episode: 38, duration: 0.213s, episode steps: 75, steps per second: 353, episode reward: 3.547, mean reward: 0.047 [-100.000, 50.587], mean action: 1.440 [0.000, 3.000], mean observation: -0.121 [-1.356, 1.952], loss: 11.357626, mean_absolute_error: 32.580154, mean_q: -41.061226\n",
      "  2930/100000: episode: 39, duration: 0.258s, episode steps: 95, steps per second: 368, episode reward: -90.116, mean reward: -0.949 [-100.000, 121.657], mean action: 0.800 [0.000, 3.000], mean observation: 0.231 [-1.872, 2.941], loss: 9.072664, mean_absolute_error: 33.321056, mean_q: -42.193607\n",
      "  3002/100000: episode: 40, duration: 0.183s, episode steps: 72, steps per second: 394, episode reward: -146.631, mean reward: -2.037 [-100.000, 7.340], mean action: 1.042 [0.000, 3.000], mean observation: 0.063 [-5.591, 1.409], loss: 17.262403, mean_absolute_error: 33.989029, mean_q: -42.776016\n",
      "  3066/100000: episode: 41, duration: 0.171s, episode steps: 64, steps per second: 375, episode reward: -26.909, mean reward: -0.420 [-100.000, 11.494], mean action: 0.891 [0.000, 3.000], mean observation: 0.049 [-1.284, 3.822], loss: 15.204903, mean_absolute_error: 33.780037, mean_q: -42.517265\n",
      "  3135/100000: episode: 42, duration: 0.180s, episode steps: 69, steps per second: 384, episode reward: -176.062, mean reward: -2.552 [-100.000, 36.270], mean action: 1.449 [0.000, 3.000], mean observation: -0.171 [-3.808, 1.406], loss: 16.541622, mean_absolute_error: 34.198357, mean_q: -42.567467\n",
      "  3189/100000: episode: 43, duration: 0.150s, episode steps: 54, steps per second: 361, episode reward: -142.515, mean reward: -2.639 [-100.000, 7.869], mean action: 0.611 [0.000, 2.000], mean observation: 0.039 [-2.655, 1.388], loss: 27.338408, mean_absolute_error: 33.957920, mean_q: -42.369011\n",
      "  3253/100000: episode: 44, duration: 0.163s, episode steps: 64, steps per second: 392, episode reward: -78.877, mean reward: -1.232 [-100.000, 30.772], mean action: 1.438 [0.000, 3.000], mean observation: -0.170 [-1.472, 3.364], loss: 16.329758, mean_absolute_error: 33.831707, mean_q: -42.492573\n",
      "  3328/100000: episode: 45, duration: 0.198s, episode steps: 75, steps per second: 379, episode reward: -101.945, mean reward: -1.359 [-100.000, 17.057], mean action: 0.827 [0.000, 3.000], mean observation: -0.048 [-1.398, 5.185], loss: 14.392069, mean_absolute_error: 34.452084, mean_q: -43.348793\n",
      "  3391/100000: episode: 46, duration: 0.157s, episode steps: 63, steps per second: 401, episode reward: -111.667, mean reward: -1.772 [-100.000, 8.716], mean action: 0.968 [0.000, 3.000], mean observation: -0.115 [-5.910, 1.404], loss: 19.979202, mean_absolute_error: 34.905647, mean_q: -44.040791\n",
      "  3454/100000: episode: 47, duration: 0.175s, episode steps: 63, steps per second: 360, episode reward: -153.115, mean reward: -2.430 [-100.000, 8.251], mean action: 1.302 [0.000, 3.000], mean observation: 0.136 [-1.444, 1.398], loss: 9.018310, mean_absolute_error: 35.086998, mean_q: -44.494896\n",
      "  3521/100000: episode: 48, duration: 0.170s, episode steps: 67, steps per second: 394, episode reward: -98.172, mean reward: -1.465 [-100.000, 10.633], mean action: 0.716 [0.000, 3.000], mean observation: 0.024 [-1.381, 5.202], loss: 11.039637, mean_absolute_error: 34.499104, mean_q: -43.709820\n",
      "  3644/100000: episode: 49, duration: 0.336s, episode steps: 123, steps per second: 366, episode reward: -354.878, mean reward: -2.885 [-100.000, 61.409], mean action: 1.276 [0.000, 3.000], mean observation: -0.112 [-2.788, 1.657], loss: 13.313362, mean_absolute_error: 35.224293, mean_q: -44.241478\n",
      "  3729/100000: episode: 50, duration: 0.215s, episode steps: 85, steps per second: 395, episode reward: -84.270, mean reward: -0.991 [-100.000, 12.836], mean action: 0.859 [0.000, 3.000], mean observation: 0.124 [-1.250, 3.423], loss: 11.655796, mean_absolute_error: 35.625473, mean_q: -44.638538\n",
      "  3801/100000: episode: 51, duration: 0.195s, episode steps: 72, steps per second: 370, episode reward: -303.160, mean reward: -4.211 [-100.000, 1.607], mean action: 1.375 [0.000, 3.000], mean observation: -0.230 [-2.148, 1.412], loss: 14.220081, mean_absolute_error: 33.855545, mean_q: -42.313419\n",
      "  3902/100000: episode: 52, duration: 0.291s, episode steps: 101, steps per second: 347, episode reward: -33.576, mean reward: -0.332 [-100.000, 9.658], mean action: 0.950 [0.000, 3.000], mean observation: 0.096 [-1.157, 2.751], loss: 16.860495, mean_absolute_error: 35.357738, mean_q: -44.467396\n",
      "  3983/100000: episode: 53, duration: 0.230s, episode steps: 81, steps per second: 353, episode reward: -55.796, mean reward: -0.689 [-100.000, 12.363], mean action: 1.444 [0.000, 3.000], mean observation: -0.050 [-0.979, 3.795], loss: 15.776230, mean_absolute_error: 34.550484, mean_q: -43.616169\n",
      "  4063/100000: episode: 54, duration: 0.243s, episode steps: 80, steps per second: 329, episode reward: -47.362, mean reward: -0.592 [-100.000, 16.388], mean action: 1.863 [0.000, 3.000], mean observation: 0.048 [-0.921, 2.815], loss: 14.977152, mean_absolute_error: 34.353554, mean_q: -43.070728\n",
      "  4197/100000: episode: 55, duration: 0.390s, episode steps: 134, steps per second: 344, episode reward: -142.079, mean reward: -1.060 [-100.000, 29.135], mean action: 1.440 [0.000, 3.000], mean observation: 0.098 [-1.681, 1.611], loss: 15.108060, mean_absolute_error: 34.591896, mean_q: -43.585751\n",
      "  4303/100000: episode: 56, duration: 0.364s, episode steps: 106, steps per second: 291, episode reward: -388.170, mean reward: -3.662 [-100.000, 41.614], mean action: 1.538 [0.000, 3.000], mean observation: -0.111 [-2.809, 1.403], loss: 13.905200, mean_absolute_error: 34.968864, mean_q: -43.777740\n",
      "  4436/100000: episode: 57, duration: 0.427s, episode steps: 133, steps per second: 311, episode reward: -170.690, mean reward: -1.283 [-100.000, 3.210], mean action: 1.338 [0.000, 3.000], mean observation: 0.220 [-0.927, 1.417], loss: 13.154308, mean_absolute_error: 34.618649, mean_q: -43.652622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4544/100000: episode: 58, duration: 0.335s, episode steps: 108, steps per second: 323, episode reward: -45.190, mean reward: -0.418 [-100.000, 16.004], mean action: 0.991 [0.000, 3.000], mean observation: -0.031 [-1.232, 2.191], loss: 13.129876, mean_absolute_error: 35.165718, mean_q: -44.561642\n",
      "  4630/100000: episode: 59, duration: 0.263s, episode steps: 86, steps per second: 327, episode reward: -108.261, mean reward: -1.259 [-100.000, 8.455], mean action: 1.384 [0.000, 3.000], mean observation: -0.017 [-1.026, 3.466], loss: 12.443233, mean_absolute_error: 35.379581, mean_q: -44.691456\n",
      "  4733/100000: episode: 60, duration: 0.332s, episode steps: 103, steps per second: 311, episode reward: -84.633, mean reward: -0.822 [-100.000, 12.727], mean action: 1.806 [0.000, 3.000], mean observation: -0.055 [-1.012, 3.853], loss: 16.306618, mean_absolute_error: 34.756184, mean_q: -43.623970\n",
      "  4798/100000: episode: 61, duration: 0.174s, episode steps: 65, steps per second: 373, episode reward: 5.033, mean reward: 0.077 [-100.000, 22.219], mean action: 1.108 [0.000, 3.000], mean observation: 0.062 [-1.314, 1.387], loss: 15.197319, mean_absolute_error: 35.151390, mean_q: -44.274742\n",
      "  4940/100000: episode: 62, duration: 0.441s, episode steps: 142, steps per second: 322, episode reward: -109.745, mean reward: -0.773 [-100.000, 5.333], mean action: 1.887 [0.000, 3.000], mean observation: 0.194 [-0.992, 1.392], loss: 17.058151, mean_absolute_error: 34.681164, mean_q: -43.424232\n",
      "  5097/100000: episode: 63, duration: 0.454s, episode steps: 157, steps per second: 346, episode reward: -161.234, mean reward: -1.027 [-100.000, 31.171], mean action: 1.892 [0.000, 3.000], mean observation: 0.165 [-2.371, 1.635], loss: 20.927645, mean_absolute_error: 33.376472, mean_q: -41.729061\n",
      "  5251/100000: episode: 64, duration: 0.421s, episode steps: 154, steps per second: 366, episode reward: -253.457, mean reward: -1.646 [-100.000, 3.055], mean action: 1.870 [0.000, 3.000], mean observation: 0.157 [-0.666, 1.409], loss: 10.999160, mean_absolute_error: 33.296970, mean_q: -41.699619\n",
      "  5326/100000: episode: 65, duration: 0.194s, episode steps: 75, steps per second: 386, episode reward: -139.597, mean reward: -1.861 [-100.000, 9.641], mean action: 1.573 [0.000, 3.000], mean observation: -0.126 [-1.570, 1.394], loss: 13.780269, mean_absolute_error: 33.081554, mean_q: -41.528313\n",
      "  5875/100000: episode: 66, duration: 1.975s, episode steps: 549, steps per second: 278, episode reward: -317.150, mean reward: -0.578 [-100.000, 8.233], mean action: 1.872 [0.000, 3.000], mean observation: 0.199 [-0.935, 1.817], loss: 13.938272, mean_absolute_error: 32.772480, mean_q: -41.280064\n",
      "  6431/100000: episode: 67, duration: 2.348s, episode steps: 556, steps per second: 237, episode reward: -267.056, mean reward: -0.480 [-100.000, 6.670], mean action: 1.773 [0.000, 3.000], mean observation: 0.211 [-0.832, 2.117], loss: 12.953566, mean_absolute_error: 32.351601, mean_q: -40.665756\n",
      "  6735/100000: episode: 68, duration: 1.039s, episode steps: 304, steps per second: 293, episode reward: -176.080, mean reward: -0.579 [-100.000, 5.859], mean action: 1.638 [0.000, 3.000], mean observation: 0.249 [-0.762, 1.835], loss: 13.938730, mean_absolute_error: 30.929157, mean_q: -38.763172\n",
      "  6888/100000: episode: 69, duration: 0.531s, episode steps: 153, steps per second: 288, episode reward: -149.954, mean reward: -0.980 [-100.000, 3.039], mean action: 1.523 [0.000, 3.000], mean observation: 0.294 [-0.633, 1.566], loss: 10.591252, mean_absolute_error: 30.295008, mean_q: -37.951397\n",
      "  7083/100000: episode: 70, duration: 0.649s, episode steps: 195, steps per second: 300, episode reward: -203.408, mean reward: -1.043 [-100.000, 4.192], mean action: 1.733 [0.000, 3.000], mean observation: 0.151 [-0.726, 1.439], loss: 7.420992, mean_absolute_error: 30.070871, mean_q: -37.841335\n",
      "  7489/100000: episode: 71, duration: 1.375s, episode steps: 406, steps per second: 295, episode reward: -171.079, mean reward: -0.421 [-100.000, 4.970], mean action: 1.512 [0.000, 3.000], mean observation: 0.206 [-0.444, 1.503], loss: 12.238346, mean_absolute_error: 29.301142, mean_q: -36.667805\n",
      "  7741/100000: episode: 72, duration: 0.842s, episode steps: 252, steps per second: 299, episode reward: -146.631, mean reward: -0.582 [-100.000, 3.680], mean action: 1.329 [0.000, 3.000], mean observation: 0.193 [-0.456, 1.446], loss: 12.857032, mean_absolute_error: 28.763382, mean_q: -36.008587\n",
      "  8129/100000: episode: 73, duration: 1.324s, episode steps: 388, steps per second: 293, episode reward: -134.091, mean reward: -0.346 [-100.000, 4.099], mean action: 1.675 [0.000, 3.000], mean observation: 0.183 [-0.541, 1.406], loss: 11.792015, mean_absolute_error: 27.621309, mean_q: -34.475872\n",
      "  8453/100000: episode: 74, duration: 1.017s, episode steps: 324, steps per second: 319, episode reward: -122.334, mean reward: -0.378 [-100.000, 4.394], mean action: 1.571 [0.000, 3.000], mean observation: 0.199 [-0.339, 1.456], loss: 9.256522, mean_absolute_error: 26.383656, mean_q: -32.925484\n",
      "  8885/100000: episode: 75, duration: 1.574s, episode steps: 432, steps per second: 274, episode reward: -246.257, mean reward: -0.570 [-100.000, 3.996], mean action: 1.380 [0.000, 3.000], mean observation: 0.210 [-0.839, 2.566], loss: 10.409728, mean_absolute_error: 25.996370, mean_q: -32.369198\n",
      "  9153/100000: episode: 76, duration: 0.886s, episode steps: 268, steps per second: 302, episode reward: -164.920, mean reward: -0.615 [-100.000, 4.592], mean action: 1.410 [0.000, 3.000], mean observation: 0.176 [-0.409, 1.506], loss: 9.351547, mean_absolute_error: 24.794994, mean_q: -30.852438\n",
      "  9558/100000: episode: 77, duration: 1.331s, episode steps: 405, steps per second: 304, episode reward: -117.005, mean reward: -0.289 [-100.000, 28.754], mean action: 1.402 [0.000, 3.000], mean observation: 0.182 [-3.123, 1.961], loss: 10.484356, mean_absolute_error: 24.420422, mean_q: -30.357498\n",
      "  9841/100000: episode: 78, duration: 0.942s, episode steps: 283, steps per second: 300, episode reward: -258.143, mean reward: -0.912 [-100.000, 4.237], mean action: 1.371 [0.000, 3.000], mean observation: 0.146 [-0.828, 2.099], loss: 7.852712, mean_absolute_error: 23.915943, mean_q: -29.805532\n",
      " 10529/100000: episode: 79, duration: 2.543s, episode steps: 688, steps per second: 271, episode reward: 119.529, mean reward: 0.174 [-17.305, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.160 [-1.128, 1.447], loss: 9.489042, mean_absolute_error: 22.823757, mean_q: -28.314243\n",
      " 10970/100000: episode: 80, duration: 1.663s, episode steps: 441, steps per second: 265, episode reward: -218.127, mean reward: -0.495 [-100.000, 4.413], mean action: 1.399 [0.000, 3.000], mean observation: 0.138 [-0.508, 1.549], loss: 7.028649, mean_absolute_error: 22.408854, mean_q: -27.658876\n",
      " 11077/100000: episode: 81, duration: 0.365s, episode steps: 107, steps per second: 293, episode reward: -190.294, mean reward: -1.778 [-100.000, 1.685], mean action: 1.318 [0.000, 3.000], mean observation: 0.169 [-0.676, 1.457], loss: 6.739160, mean_absolute_error: 21.819162, mean_q: -26.736942\n",
      " 11801/100000: episode: 82, duration: 2.803s, episode steps: 724, steps per second: 258, episode reward: -214.319, mean reward: -0.296 [-100.000, 3.852], mean action: 1.561 [0.000, 3.000], mean observation: 0.073 [-1.003, 1.468], loss: 10.073418, mean_absolute_error: 21.225229, mean_q: -25.830568\n",
      " 12801/100000: episode: 83, duration: 4.454s, episode steps: 1000, steps per second: 225, episode reward: -73.745, mean reward: -0.074 [-4.277, 4.577], mean action: 1.845 [0.000, 3.000], mean observation: -0.024 [-0.948, 1.404], loss: 11.323427, mean_absolute_error: 18.745995, mean_q: -22.168602\n",
      " 13083/100000: episode: 84, duration: 0.963s, episode steps: 282, steps per second: 293, episode reward: -73.010, mean reward: -0.259 [-100.000, 12.726], mean action: 1.681 [0.000, 3.000], mean observation: 0.100 [-0.718, 1.416], loss: 9.274975, mean_absolute_error: 17.298672, mean_q: -19.906290\n",
      " 14083/100000: episode: 85, duration: 5.318s, episode steps: 1000, steps per second: 188, episode reward: -40.542, mean reward: -0.041 [-4.943, 4.476], mean action: 1.780 [0.000, 3.000], mean observation: 0.055 [-0.301, 1.543], loss: 9.704279, mean_absolute_error: 16.474125, mean_q: -18.466440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14304/100000: episode: 86, duration: 0.724s, episode steps: 221, steps per second: 305, episode reward: -89.323, mean reward: -0.404 [-100.000, 13.347], mean action: 1.516 [0.000, 3.000], mean observation: 0.116 [-1.408, 1.406], loss: 6.861284, mean_absolute_error: 15.231625, mean_q: -16.402731\n",
      " 14531/100000: episode: 87, duration: 0.753s, episode steps: 227, steps per second: 301, episode reward: -147.033, mean reward: -0.648 [-100.000, 3.489], mean action: 1.454 [0.000, 3.000], mean observation: 0.169 [-0.410, 1.400], loss: 10.678433, mean_absolute_error: 15.028285, mean_q: -16.116560\n",
      " 14684/100000: episode: 88, duration: 0.532s, episode steps: 153, steps per second: 288, episode reward: -153.772, mean reward: -1.005 [-100.000, 3.151], mean action: 1.510 [0.000, 3.000], mean observation: 0.175 [-0.496, 1.414], loss: 6.163798, mean_absolute_error: 15.888356, mean_q: -16.968409\n",
      " 14892/100000: episode: 89, duration: 0.684s, episode steps: 208, steps per second: 304, episode reward: -150.780, mean reward: -0.725 [-100.000, 12.289], mean action: 1.399 [0.000, 3.000], mean observation: 0.155 [-1.151, 1.457], loss: 9.308938, mean_absolute_error: 15.044455, mean_q: -15.502880\n",
      " 15137/100000: episode: 90, duration: 0.735s, episode steps: 245, steps per second: 333, episode reward: -35.760, mean reward: -0.146 [-100.000, 13.959], mean action: 1.678 [0.000, 3.000], mean observation: 0.102 [-0.607, 1.452], loss: 8.846023, mean_absolute_error: 14.871812, mean_q: -15.172834\n",
      " 16137/100000: episode: 91, duration: 4.063s, episode steps: 1000, steps per second: 246, episode reward: -26.182, mean reward: -0.026 [-5.622, 5.102], mean action: 1.662 [0.000, 3.000], mean observation: 0.035 [-0.558, 1.416], loss: 11.024292, mean_absolute_error: 14.779285, mean_q: -14.057065\n",
      " 16543/100000: episode: 92, duration: 1.563s, episode steps: 406, steps per second: 260, episode reward: -269.163, mean reward: -0.663 [-100.000, 5.061], mean action: 1.813 [0.000, 3.000], mean observation: 0.046 [-1.621, 1.390], loss: 9.308103, mean_absolute_error: 14.643074, mean_q: -12.821734\n",
      " 16968/100000: episode: 93, duration: 1.518s, episode steps: 425, steps per second: 280, episode reward: -144.907, mean reward: -0.341 [-100.000, 18.288], mean action: 1.713 [0.000, 3.000], mean observation: 0.074 [-1.922, 1.421], loss: 10.552858, mean_absolute_error: 14.496921, mean_q: -11.762184\n",
      " 17261/100000: episode: 94, duration: 0.997s, episode steps: 293, steps per second: 294, episode reward: -139.610, mean reward: -0.476 [-100.000, 7.502], mean action: 1.553 [0.000, 3.000], mean observation: 0.151 [-1.176, 1.404], loss: 5.979619, mean_absolute_error: 14.357225, mean_q: -10.881024\n",
      " 17411/100000: episode: 95, duration: 0.489s, episode steps: 150, steps per second: 306, episode reward: -94.912, mean reward: -0.633 [-100.000, 3.308], mean action: 1.593 [0.000, 3.000], mean observation: 0.225 [-0.570, 1.440], loss: 10.385694, mean_absolute_error: 14.726387, mean_q: -11.037360\n",
      " 17689/100000: episode: 96, duration: 0.910s, episode steps: 278, steps per second: 306, episode reward: -134.024, mean reward: -0.482 [-100.000, 13.769], mean action: 1.637 [0.000, 3.000], mean observation: 0.138 [-1.088, 1.476], loss: 10.659253, mean_absolute_error: 14.472359, mean_q: -9.888359\n",
      " 17909/100000: episode: 97, duration: 0.710s, episode steps: 220, steps per second: 310, episode reward: -83.590, mean reward: -0.380 [-100.000, 11.663], mean action: 1.745 [0.000, 3.000], mean observation: 0.113 [-0.552, 1.438], loss: 10.002746, mean_absolute_error: 15.079682, mean_q: -9.771470\n",
      " 18026/100000: episode: 98, duration: 0.374s, episode steps: 117, steps per second: 313, episode reward: -108.955, mean reward: -0.931 [-100.000, 3.078], mean action: 1.709 [0.000, 3.000], mean observation: 0.202 [-0.771, 1.400], loss: 8.239174, mean_absolute_error: 14.932890, mean_q: -9.105667\n",
      " 18163/100000: episode: 99, duration: 0.432s, episode steps: 137, steps per second: 317, episode reward: -109.845, mean reward: -0.802 [-100.000, 16.097], mean action: 1.635 [0.000, 3.000], mean observation: 0.119 [-0.813, 1.934], loss: 6.906115, mean_absolute_error: 14.719670, mean_q: -8.094752\n",
      " 19163/100000: episode: 100, duration: 4.602s, episode steps: 1000, steps per second: 217, episode reward: -79.971, mean reward: -0.080 [-5.781, 4.971], mean action: 1.712 [0.000, 3.000], mean observation: 0.081 [-0.985, 1.398], loss: 8.028868, mean_absolute_error: 15.298758, mean_q: -7.693357\n",
      " 19315/100000: episode: 101, duration: 0.467s, episode steps: 152, steps per second: 325, episode reward: -112.465, mean reward: -0.740 [-100.000, 12.282], mean action: 1.579 [0.000, 3.000], mean observation: 0.138 [-0.841, 1.475], loss: 5.538176, mean_absolute_error: 15.745212, mean_q: -7.051923\n",
      " 20018/100000: episode: 102, duration: 2.794s, episode steps: 703, steps per second: 252, episode reward: -201.439, mean reward: -0.287 [-100.000, 4.703], mean action: 1.683 [0.000, 3.000], mean observation: 0.118 [-0.564, 1.400], loss: 8.495162, mean_absolute_error: 16.268684, mean_q: -6.842782\n",
      " 20828/100000: episode: 103, duration: 3.858s, episode steps: 810, steps per second: 210, episode reward: -202.624, mean reward: -0.250 [-100.000, 5.219], mean action: 1.657 [0.000, 3.000], mean observation: 0.104 [-0.544, 1.391], loss: 8.511989, mean_absolute_error: 16.876289, mean_q: -5.851711\n",
      " 21273/100000: episode: 104, duration: 1.548s, episode steps: 445, steps per second: 287, episode reward: -193.309, mean reward: -0.434 [-100.000, 4.630], mean action: 1.816 [0.000, 3.000], mean observation: 0.156 [-0.632, 1.409], loss: 9.180289, mean_absolute_error: 17.222546, mean_q: -5.402975\n",
      " 21454/100000: episode: 105, duration: 0.573s, episode steps: 181, steps per second: 316, episode reward: -107.208, mean reward: -0.592 [-100.000, 13.346], mean action: 1.519 [0.000, 3.000], mean observation: 0.193 [-0.824, 1.443], loss: 7.776674, mean_absolute_error: 17.197428, mean_q: -4.259751\n",
      " 21916/100000: episode: 106, duration: 1.793s, episode steps: 462, steps per second: 258, episode reward: -156.275, mean reward: -0.338 [-100.000, 3.413], mean action: 1.649 [0.000, 3.000], mean observation: 0.182 [-0.452, 1.424], loss: 6.868919, mean_absolute_error: 17.663675, mean_q: -4.822948\n",
      " 22458/100000: episode: 107, duration: 2.035s, episode steps: 542, steps per second: 266, episode reward: -111.688, mean reward: -0.206 [-100.000, 4.428], mean action: 1.904 [0.000, 3.000], mean observation: 0.206 [-0.725, 1.772], loss: 7.736715, mean_absolute_error: 17.874182, mean_q: -4.101396\n",
      " 22879/100000: episode: 108, duration: 1.576s, episode steps: 421, steps per second: 267, episode reward: -116.145, mean reward: -0.276 [-100.000, 4.664], mean action: 1.675 [0.000, 3.000], mean observation: 0.196 [-0.490, 1.414], loss: 7.584336, mean_absolute_error: 18.398369, mean_q: -4.026633\n",
      " 23026/100000: episode: 109, duration: 0.508s, episode steps: 147, steps per second: 290, episode reward: -106.525, mean reward: -0.725 [-100.000, 2.664], mean action: 1.680 [0.000, 3.000], mean observation: 0.232 [-0.725, 1.469], loss: 8.452525, mean_absolute_error: 18.244257, mean_q: -2.827218\n",
      " 24026/100000: episode: 110, duration: 3.935s, episode steps: 1000, steps per second: 254, episode reward: -81.095, mean reward: -0.081 [-4.492, 4.501], mean action: 1.876 [0.000, 3.000], mean observation: 0.176 [-0.482, 1.389], loss: 8.175758, mean_absolute_error: 18.612661, mean_q: -3.139200\n",
      " 25026/100000: episode: 111, duration: 4.199s, episode steps: 1000, steps per second: 238, episode reward: -127.391, mean reward: -0.127 [-4.635, 4.023], mean action: 1.782 [0.000, 3.000], mean observation: 0.180 [-0.478, 1.401], loss: 7.898083, mean_absolute_error: 18.580494, mean_q: -1.162460\n",
      " 26026/100000: episode: 112, duration: 4.424s, episode steps: 1000, steps per second: 226, episode reward: -133.650, mean reward: -0.134 [-4.606, 4.689], mean action: 1.780 [0.000, 3.000], mean observation: 0.135 [-0.373, 1.395], loss: 6.475836, mean_absolute_error: 19.043806, mean_q: 1.110793\n",
      " 27026/100000: episode: 113, duration: 5.520s, episode steps: 1000, steps per second: 181, episode reward: -145.174, mean reward: -0.145 [-5.563, 3.859], mean action: 1.783 [0.000, 3.000], mean observation: 0.142 [-0.511, 1.424], loss: 7.246543, mean_absolute_error: 19.529520, mean_q: 3.196913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28026/100000: episode: 114, duration: 5.293s, episode steps: 1000, steps per second: 189, episode reward: -56.788, mean reward: -0.057 [-4.835, 4.611], mean action: 1.841 [0.000, 3.000], mean observation: 0.184 [-0.752, 1.398], loss: 5.450135, mean_absolute_error: 19.805973, mean_q: 4.397847\n",
      " 29026/100000: episode: 115, duration: 5.035s, episode steps: 1000, steps per second: 199, episode reward: -57.279, mean reward: -0.057 [-4.611, 4.299], mean action: 1.709 [0.000, 3.000], mean observation: 0.162 [-0.222, 1.410], loss: 6.604644, mean_absolute_error: 20.053078, mean_q: 5.953378\n",
      " 30026/100000: episode: 116, duration: 6.097s, episode steps: 1000, steps per second: 164, episode reward: -69.007, mean reward: -0.069 [-4.855, 4.505], mean action: 1.750 [0.000, 3.000], mean observation: 0.095 [-0.441, 1.433], loss: 7.142855, mean_absolute_error: 19.761776, mean_q: 8.204585\n",
      " 31026/100000: episode: 117, duration: 4.343s, episode steps: 1000, steps per second: 230, episode reward: -119.015, mean reward: -0.119 [-11.661, 10.072], mean action: 1.656 [0.000, 3.000], mean observation: 0.004 [-0.844, 1.400], loss: 7.580147, mean_absolute_error: 20.150270, mean_q: 9.556746\n",
      " 31653/100000: episode: 118, duration: 2.799s, episode steps: 627, steps per second: 224, episode reward: -85.097, mean reward: -0.136 [-100.000, 17.657], mean action: 1.684 [0.000, 3.000], mean observation: 0.012 [-0.909, 1.388], loss: 6.781472, mean_absolute_error: 20.312433, mean_q: 10.935668\n",
      " 32653/100000: episode: 119, duration: 4.579s, episode steps: 1000, steps per second: 218, episode reward: -63.705, mean reward: -0.064 [-6.060, 5.273], mean action: 1.679 [0.000, 3.000], mean observation: 0.088 [-0.777, 1.583], loss: 6.870444, mean_absolute_error: 20.869249, mean_q: 12.245646\n",
      " 33653/100000: episode: 120, duration: 4.696s, episode steps: 1000, steps per second: 213, episode reward: -70.051, mean reward: -0.070 [-6.393, 4.804], mean action: 1.875 [0.000, 3.000], mean observation: 0.052 [-0.831, 1.387], loss: 6.278618, mean_absolute_error: 21.314024, mean_q: 14.710403\n",
      " 34653/100000: episode: 121, duration: 4.522s, episode steps: 1000, steps per second: 221, episode reward: -95.823, mean reward: -0.096 [-5.161, 4.370], mean action: 1.863 [0.000, 3.000], mean observation: 0.036 [-0.608, 1.465], loss: 7.685764, mean_absolute_error: 22.088984, mean_q: 15.997646\n",
      " 35653/100000: episode: 122, duration: 4.584s, episode steps: 1000, steps per second: 218, episode reward: -56.015, mean reward: -0.056 [-4.513, 4.748], mean action: 1.781 [0.000, 3.000], mean observation: 0.007 [-0.803, 1.424], loss: 6.782063, mean_absolute_error: 22.719503, mean_q: 17.597979\n",
      " 36653/100000: episode: 123, duration: 5.073s, episode steps: 1000, steps per second: 197, episode reward: -105.053, mean reward: -0.105 [-4.629, 3.746], mean action: 1.744 [0.000, 3.000], mean observation: 0.004 [-0.734, 1.412], loss: 5.966731, mean_absolute_error: 23.636215, mean_q: 19.035038\n",
      " 37653/100000: episode: 124, duration: 5.065s, episode steps: 1000, steps per second: 197, episode reward: -70.946, mean reward: -0.071 [-4.874, 4.588], mean action: 1.950 [0.000, 3.000], mean observation: 0.022 [-0.935, 1.401], loss: 6.091813, mean_absolute_error: 23.916605, mean_q: 20.013523\n",
      " 38653/100000: episode: 125, duration: 4.173s, episode steps: 1000, steps per second: 240, episode reward: -80.850, mean reward: -0.081 [-4.991, 4.254], mean action: 1.932 [0.000, 3.000], mean observation: 0.053 [-0.988, 1.407], loss: 5.418547, mean_absolute_error: 24.083330, mean_q: 20.980036\n",
      " 39653/100000: episode: 126, duration: 5.116s, episode steps: 1000, steps per second: 195, episode reward: -43.394, mean reward: -0.043 [-4.925, 6.423], mean action: 1.806 [0.000, 3.000], mean observation: 0.016 [-0.754, 1.491], loss: 5.936314, mean_absolute_error: 24.654455, mean_q: 21.329859\n",
      " 40653/100000: episode: 127, duration: 4.437s, episode steps: 1000, steps per second: 225, episode reward: -15.119, mean reward: -0.015 [-5.266, 5.480], mean action: 1.943 [0.000, 3.000], mean observation: 0.018 [-0.783, 1.411], loss: 5.757663, mean_absolute_error: 25.400331, mean_q: 22.445559\n",
      " 41653/100000: episode: 128, duration: 4.712s, episode steps: 1000, steps per second: 212, episode reward: -55.343, mean reward: -0.055 [-4.095, 4.318], mean action: 1.910 [0.000, 3.000], mean observation: -0.010 [-0.693, 1.409], loss: 7.171617, mean_absolute_error: 26.139076, mean_q: 24.155327\n",
      " 42428/100000: episode: 129, duration: 3.355s, episode steps: 775, steps per second: 231, episode reward: -119.750, mean reward: -0.155 [-100.000, 11.032], mean action: 1.810 [0.000, 3.000], mean observation: 0.003 [-0.859, 1.487], loss: 5.834237, mean_absolute_error: 26.666096, mean_q: 25.559189\n",
      " 43428/100000: episode: 130, duration: 4.311s, episode steps: 1000, steps per second: 232, episode reward: -74.372, mean reward: -0.074 [-4.393, 4.552], mean action: 1.886 [0.000, 3.000], mean observation: -0.009 [-0.709, 1.491], loss: 6.156122, mean_absolute_error: 27.217237, mean_q: 26.851938\n",
      " 44360/100000: episode: 131, duration: 5.314s, episode steps: 932, steps per second: 175, episode reward: 190.399, mean reward: 0.204 [-21.273, 100.000], mean action: 1.643 [0.000, 3.000], mean observation: 0.119 [-0.610, 1.386], loss: 5.468780, mean_absolute_error: 27.882282, mean_q: 28.269789\n",
      " 44894/100000: episode: 132, duration: 2.080s, episode steps: 534, steps per second: 257, episode reward: -96.419, mean reward: -0.181 [-100.000, 15.081], mean action: 1.850 [0.000, 3.000], mean observation: -0.014 [-0.748, 1.389], loss: 4.995937, mean_absolute_error: 28.792387, mean_q: 29.082729\n",
      " 45894/100000: episode: 133, duration: 4.187s, episode steps: 1000, steps per second: 239, episode reward: -34.381, mean reward: -0.034 [-11.443, 11.185], mean action: 1.750 [0.000, 3.000], mean observation: 0.060 [-0.758, 1.491], loss: 5.784762, mean_absolute_error: 29.331238, mean_q: 29.988577\n",
      " 46759/100000: episode: 134, duration: 3.951s, episode steps: 865, steps per second: 219, episode reward: -146.489, mean reward: -0.169 [-100.000, 15.640], mean action: 1.749 [0.000, 3.000], mean observation: 0.001 [-0.779, 1.481], loss: 5.683913, mean_absolute_error: 29.726732, mean_q: 30.193983\n",
      " 47518/100000: episode: 135, duration: 3.298s, episode steps: 759, steps per second: 230, episode reward: -142.016, mean reward: -0.187 [-100.000, 10.881], mean action: 1.930 [0.000, 3.000], mean observation: 0.025 [-0.633, 1.411], loss: 6.532439, mean_absolute_error: 30.437965, mean_q: 31.676056\n",
      " 48518/100000: episode: 136, duration: 4.110s, episode steps: 1000, steps per second: 243, episode reward: -86.409, mean reward: -0.086 [-4.531, 4.305], mean action: 1.756 [0.000, 3.000], mean observation: 0.197 [-0.416, 1.402], loss: 4.965490, mean_absolute_error: 29.988361, mean_q: 31.517107\n",
      " 49518/100000: episode: 137, duration: 3.911s, episode steps: 1000, steps per second: 256, episode reward: -29.744, mean reward: -0.030 [-4.367, 4.654], mean action: 1.928 [0.000, 3.000], mean observation: -0.019 [-0.865, 1.388], loss: 6.410211, mean_absolute_error: 30.096498, mean_q: 31.577869\n",
      " 50518/100000: episode: 138, duration: 4.478s, episode steps: 1000, steps per second: 223, episode reward: -70.659, mean reward: -0.071 [-5.063, 4.147], mean action: 1.747 [0.000, 3.000], mean observation: 0.134 [-0.478, 1.409], loss: 6.696758, mean_absolute_error: 29.828896, mean_q: 31.729351\n",
      " 51518/100000: episode: 139, duration: 4.679s, episode steps: 1000, steps per second: 214, episode reward: -101.469, mean reward: -0.101 [-4.382, 4.189], mean action: 1.804 [0.000, 3.000], mean observation: 0.148 [-0.616, 1.420], loss: 6.140428, mean_absolute_error: 29.728552, mean_q: 32.031292\n",
      " 52518/100000: episode: 140, duration: 4.033s, episode steps: 1000, steps per second: 248, episode reward: -89.358, mean reward: -0.089 [-4.941, 4.639], mean action: 1.810 [0.000, 3.000], mean observation: 0.154 [-0.301, 1.398], loss: 5.210424, mean_absolute_error: 29.696190, mean_q: 31.941168\n",
      " 53518/100000: episode: 141, duration: 4.497s, episode steps: 1000, steps per second: 222, episode reward: -95.277, mean reward: -0.095 [-4.659, 4.587], mean action: 1.920 [0.000, 3.000], mean observation: 0.145 [-0.242, 1.431], loss: 5.574154, mean_absolute_error: 30.206347, mean_q: 32.587616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54518/100000: episode: 142, duration: 4.108s, episode steps: 1000, steps per second: 243, episode reward: -88.897, mean reward: -0.089 [-4.339, 4.849], mean action: 1.704 [0.000, 3.000], mean observation: 0.160 [-0.281, 1.398], loss: 6.967619, mean_absolute_error: 30.415562, mean_q: 32.640732\n",
      " 55518/100000: episode: 143, duration: 3.851s, episode steps: 1000, steps per second: 260, episode reward: -82.119, mean reward: -0.082 [-4.736, 5.193], mean action: 1.794 [0.000, 3.000], mean observation: 0.175 [-0.234, 1.462], loss: 5.478444, mean_absolute_error: 30.530975, mean_q: 32.925949\n",
      " 56518/100000: episode: 144, duration: 5.473s, episode steps: 1000, steps per second: 183, episode reward: -69.291, mean reward: -0.069 [-4.595, 5.407], mean action: 1.821 [0.000, 3.000], mean observation: 0.050 [-0.500, 1.401], loss: 6.984499, mean_absolute_error: 30.784012, mean_q: 33.945187\n",
      " 57518/100000: episode: 145, duration: 4.527s, episode steps: 1000, steps per second: 221, episode reward: -70.083, mean reward: -0.070 [-4.923, 5.257], mean action: 1.856 [0.000, 3.000], mean observation: 0.124 [-0.482, 1.424], loss: 5.860092, mean_absolute_error: 31.112961, mean_q: 34.689713\n",
      " 58518/100000: episode: 146, duration: 4.856s, episode steps: 1000, steps per second: 206, episode reward: -78.406, mean reward: -0.078 [-3.657, 4.326], mean action: 1.923 [0.000, 3.000], mean observation: 0.128 [-0.413, 1.485], loss: 6.300719, mean_absolute_error: 31.631582, mean_q: 35.418312\n",
      " 59518/100000: episode: 147, duration: 5.357s, episode steps: 1000, steps per second: 187, episode reward: -80.000, mean reward: -0.080 [-3.510, 5.003], mean action: 1.779 [0.000, 3.000], mean observation: 0.075 [-0.395, 1.425], loss: 5.666874, mean_absolute_error: 31.881762, mean_q: 36.135429\n",
      " 60518/100000: episode: 148, duration: 4.512s, episode steps: 1000, steps per second: 222, episode reward: -24.048, mean reward: -0.024 [-3.441, 4.454], mean action: 1.719 [0.000, 3.000], mean observation: 0.143 [-0.420, 1.804], loss: 6.442903, mean_absolute_error: 31.854273, mean_q: 35.623520\n",
      " 61303/100000: episode: 149, duration: 3.483s, episode steps: 785, steps per second: 225, episode reward: 200.700, mean reward: 0.256 [-21.503, 100.000], mean action: 1.534 [0.000, 3.000], mean observation: 0.053 [-0.726, 1.400], loss: 7.311992, mean_absolute_error: 31.836590, mean_q: 35.954384\n",
      " 62119/100000: episode: 150, duration: 3.079s, episode steps: 816, steps per second: 265, episode reward: 164.599, mean reward: 0.202 [-14.832, 100.000], mean action: 1.681 [0.000, 3.000], mean observation: 0.025 [-0.937, 1.393], loss: 7.500578, mean_absolute_error: 31.786880, mean_q: 34.957253\n",
      " 63008/100000: episode: 151, duration: 3.336s, episode steps: 889, steps per second: 266, episode reward: -69.267, mean reward: -0.078 [-100.000, 13.027], mean action: 1.754 [0.000, 3.000], mean observation: 0.007 [-0.778, 1.413], loss: 6.952386, mean_absolute_error: 31.636337, mean_q: 35.624584\n",
      " 64008/100000: episode: 152, duration: 4.225s, episode steps: 1000, steps per second: 237, episode reward: -46.560, mean reward: -0.047 [-5.078, 5.085], mean action: 1.633 [0.000, 3.000], mean observation: 0.110 [-0.459, 1.803], loss: 5.690133, mean_absolute_error: 31.223703, mean_q: 35.324764\n",
      " 65008/100000: episode: 153, duration: 5.687s, episode steps: 1000, steps per second: 176, episode reward: -112.894, mean reward: -0.113 [-4.641, 4.314], mean action: 1.778 [0.000, 3.000], mean observation: 0.013 [-0.927, 1.391], loss: 6.344608, mean_absolute_error: 31.310968, mean_q: 35.169029\n",
      " 66008/100000: episode: 154, duration: 4.752s, episode steps: 1000, steps per second: 210, episode reward: -126.811, mean reward: -0.127 [-5.145, 4.644], mean action: 1.802 [0.000, 3.000], mean observation: 0.033 [-0.780, 1.403], loss: 5.725033, mean_absolute_error: 30.815699, mean_q: 35.362988\n",
      " 67008/100000: episode: 155, duration: 4.259s, episode steps: 1000, steps per second: 235, episode reward: -88.757, mean reward: -0.089 [-4.916, 4.878], mean action: 1.798 [0.000, 3.000], mean observation: 0.004 [-0.887, 1.481], loss: 6.153331, mean_absolute_error: 31.096266, mean_q: 35.470921\n",
      " 68008/100000: episode: 156, duration: 3.881s, episode steps: 1000, steps per second: 258, episode reward: -139.595, mean reward: -0.140 [-5.216, 3.980], mean action: 1.742 [0.000, 3.000], mean observation: 0.014 [-0.904, 1.428], loss: 6.190353, mean_absolute_error: 31.077070, mean_q: 35.775703\n",
      " 69008/100000: episode: 157, duration: 4.451s, episode steps: 1000, steps per second: 225, episode reward: -87.259, mean reward: -0.087 [-5.690, 4.488], mean action: 1.588 [0.000, 3.000], mean observation: 0.059 [-0.793, 1.467], loss: 6.179906, mean_absolute_error: 31.343693, mean_q: 35.948795\n",
      " 70008/100000: episode: 158, duration: 5.562s, episode steps: 1000, steps per second: 180, episode reward: -95.609, mean reward: -0.096 [-4.737, 4.461], mean action: 1.737 [0.000, 3.000], mean observation: 0.029 [-0.793, 1.388], loss: 5.607936, mean_absolute_error: 31.423798, mean_q: 36.382061\n",
      " 71008/100000: episode: 159, duration: 4.823s, episode steps: 1000, steps per second: 207, episode reward: -123.164, mean reward: -0.123 [-6.300, 4.799], mean action: 1.678 [0.000, 3.000], mean observation: 0.050 [-0.810, 1.402], loss: 5.368608, mean_absolute_error: 31.338203, mean_q: 36.433949\n",
      " 72008/100000: episode: 160, duration: 4.266s, episode steps: 1000, steps per second: 234, episode reward: -112.731, mean reward: -0.113 [-5.410, 4.374], mean action: 1.827 [0.000, 3.000], mean observation: 0.061 [-0.729, 1.388], loss: 7.344621, mean_absolute_error: 31.541780, mean_q: 36.595711\n",
      " 73008/100000: episode: 161, duration: 4.149s, episode steps: 1000, steps per second: 241, episode reward: -108.160, mean reward: -0.108 [-5.413, 5.235], mean action: 1.727 [0.000, 3.000], mean observation: 0.070 [-0.599, 1.408], loss: 4.812189, mean_absolute_error: 31.275219, mean_q: 36.129520\n",
      " 74008/100000: episode: 162, duration: 4.353s, episode steps: 1000, steps per second: 230, episode reward: -86.479, mean reward: -0.086 [-5.144, 3.999], mean action: 1.863 [0.000, 3.000], mean observation: 0.027 [-0.617, 1.394], loss: 6.089444, mean_absolute_error: 31.277340, mean_q: 36.580704\n",
      " 74618/100000: episode: 163, duration: 2.658s, episode steps: 610, steps per second: 229, episode reward: -153.357, mean reward: -0.251 [-100.000, 11.921], mean action: 1.416 [0.000, 3.000], mean observation: 0.181 [-0.582, 1.816], loss: 5.218311, mean_absolute_error: 31.031633, mean_q: 36.966110\n",
      " 75618/100000: episode: 164, duration: 4.398s, episode steps: 1000, steps per second: 227, episode reward: -86.220, mean reward: -0.086 [-5.404, 4.228], mean action: 1.838 [0.000, 3.000], mean observation: 0.033 [-0.612, 1.424], loss: 5.115282, mean_absolute_error: 31.088692, mean_q: 36.137897\n",
      " 76618/100000: episode: 165, duration: 4.924s, episode steps: 1000, steps per second: 203, episode reward: -52.729, mean reward: -0.053 [-5.658, 4.583], mean action: 1.777 [0.000, 3.000], mean observation: 0.072 [-0.742, 1.475], loss: 5.124929, mean_absolute_error: 30.989021, mean_q: 36.096245\n",
      " 77618/100000: episode: 166, duration: 4.725s, episode steps: 1000, steps per second: 212, episode reward: -86.147, mean reward: -0.086 [-4.905, 4.822], mean action: 1.850 [0.000, 3.000], mean observation: 0.042 [-0.502, 1.394], loss: 5.848040, mean_absolute_error: 30.612976, mean_q: 35.559586\n",
      " 78618/100000: episode: 167, duration: 4.906s, episode steps: 1000, steps per second: 204, episode reward: -78.983, mean reward: -0.079 [-4.306, 4.754], mean action: 1.853 [0.000, 3.000], mean observation: 0.026 [-0.635, 1.479], loss: 5.137397, mean_absolute_error: 30.233871, mean_q: 35.205563\n",
      " 79618/100000: episode: 168, duration: 5.308s, episode steps: 1000, steps per second: 188, episode reward: -65.100, mean reward: -0.065 [-4.946, 4.440], mean action: 1.920 [0.000, 3.000], mean observation: 0.023 [-0.517, 1.461], loss: 5.753838, mean_absolute_error: 29.885485, mean_q: 34.768570\n",
      " 80618/100000: episode: 169, duration: 4.602s, episode steps: 1000, steps per second: 217, episode reward: -115.262, mean reward: -0.115 [-5.783, 4.746], mean action: 1.886 [0.000, 3.000], mean observation: 0.059 [-0.500, 1.411], loss: 5.206078, mean_absolute_error: 29.202662, mean_q: 34.283085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81618/100000: episode: 170, duration: 4.155s, episode steps: 1000, steps per second: 241, episode reward: -71.033, mean reward: -0.071 [-4.652, 4.200], mean action: 1.420 [0.000, 3.000], mean observation: 0.184 [-0.152, 1.406], loss: 4.857139, mean_absolute_error: 28.906933, mean_q: 33.595646\n",
      " 82618/100000: episode: 171, duration: 4.774s, episode steps: 1000, steps per second: 209, episode reward: -74.445, mean reward: -0.074 [-5.145, 4.943], mean action: 1.718 [0.000, 3.000], mean observation: 0.102 [-0.786, 1.412], loss: 4.999288, mean_absolute_error: 28.285997, mean_q: 32.568596\n",
      " 83618/100000: episode: 172, duration: 3.933s, episode steps: 1000, steps per second: 254, episode reward: -92.447, mean reward: -0.092 [-4.959, 4.638], mean action: 1.513 [0.000, 3.000], mean observation: 0.221 [-0.157, 1.413], loss: 4.790727, mean_absolute_error: 27.500744, mean_q: 31.580008\n",
      " 84618/100000: episode: 173, duration: 4.514s, episode steps: 1000, steps per second: 222, episode reward: -68.476, mean reward: -0.068 [-6.131, 4.213], mean action: 1.750 [0.000, 3.000], mean observation: 0.127 [-0.509, 1.401], loss: 4.346456, mean_absolute_error: 27.005449, mean_q: 30.720367\n",
      " 85618/100000: episode: 174, duration: 4.076s, episode steps: 1000, steps per second: 245, episode reward: -28.349, mean reward: -0.028 [-4.547, 4.550], mean action: 1.746 [0.000, 3.000], mean observation: 0.160 [-0.155, 1.438], loss: 4.710253, mean_absolute_error: 26.561031, mean_q: 29.768984\n",
      " 86618/100000: episode: 175, duration: 5.415s, episode steps: 1000, steps per second: 185, episode reward: -82.163, mean reward: -0.082 [-4.676, 4.374], mean action: 1.767 [0.000, 3.000], mean observation: 0.082 [-0.356, 1.415], loss: 3.906773, mean_absolute_error: 26.006262, mean_q: 29.898558\n",
      " 87618/100000: episode: 176, duration: 4.512s, episode steps: 1000, steps per second: 222, episode reward: -89.022, mean reward: -0.089 [-4.582, 4.239], mean action: 1.521 [0.000, 3.000], mean observation: 0.212 [-0.154, 1.428], loss: 5.641367, mean_absolute_error: 25.291706, mean_q: 29.360914\n",
      " 88618/100000: episode: 177, duration: 4.175s, episode steps: 1000, steps per second: 240, episode reward: -72.805, mean reward: -0.073 [-4.835, 4.115], mean action: 1.446 [0.000, 3.000], mean observation: 0.197 [-0.492, 1.391], loss: 4.788256, mean_absolute_error: 24.853456, mean_q: 28.468950\n",
      " 89618/100000: episode: 178, duration: 4.596s, episode steps: 1000, steps per second: 218, episode reward: -126.254, mean reward: -0.126 [-5.220, 4.271], mean action: 1.533 [0.000, 3.000], mean observation: 0.216 [-0.178, 1.419], loss: 4.995070, mean_absolute_error: 24.568985, mean_q: 28.229349\n",
      " 90618/100000: episode: 179, duration: 3.931s, episode steps: 1000, steps per second: 254, episode reward: -60.030, mean reward: -0.060 [-4.526, 4.045], mean action: 1.498 [0.000, 3.000], mean observation: 0.211 [-0.203, 1.503], loss: 5.472394, mean_absolute_error: 24.341696, mean_q: 28.213938\n",
      " 91618/100000: episode: 180, duration: 4.795s, episode steps: 1000, steps per second: 209, episode reward: -37.939, mean reward: -0.038 [-5.436, 5.071], mean action: 1.771 [0.000, 3.000], mean observation: 0.049 [-0.409, 1.392], loss: 4.942705, mean_absolute_error: 23.741444, mean_q: 27.296398\n",
      " 92618/100000: episode: 181, duration: 4.454s, episode steps: 1000, steps per second: 225, episode reward: -93.901, mean reward: -0.094 [-4.612, 4.142], mean action: 1.496 [0.000, 3.000], mean observation: 0.219 [-0.181, 1.413], loss: 3.752241, mean_absolute_error: 23.321747, mean_q: 26.967602\n",
      " 93618/100000: episode: 182, duration: 5.594s, episode steps: 1000, steps per second: 179, episode reward: -122.804, mean reward: -0.123 [-2.995, 3.713], mean action: 1.779 [0.000, 3.000], mean observation: 0.156 [-0.143, 1.413], loss: 4.691291, mean_absolute_error: 23.287024, mean_q: 26.646002\n",
      " 94618/100000: episode: 183, duration: 4.243s, episode steps: 1000, steps per second: 236, episode reward: -92.178, mean reward: -0.092 [-4.567, 4.551], mean action: 1.489 [0.000, 3.000], mean observation: 0.169 [-0.639, 1.416], loss: 4.922724, mean_absolute_error: 23.135502, mean_q: 26.608011\n",
      " 95529/100000: episode: 184, duration: 3.581s, episode steps: 911, steps per second: 254, episode reward: -200.169, mean reward: -0.220 [-100.000, 4.313], mean action: 1.452 [0.000, 3.000], mean observation: 0.218 [-0.251, 1.429], loss: 4.015045, mean_absolute_error: 22.721210, mean_q: 26.369846\n",
      " 96529/100000: episode: 185, duration: 4.543s, episode steps: 1000, steps per second: 220, episode reward: -131.085, mean reward: -0.131 [-4.571, 4.299], mean action: 1.435 [0.000, 3.000], mean observation: 0.198 [-0.296, 1.400], loss: 3.803259, mean_absolute_error: 22.311029, mean_q: 26.064915\n",
      " 97529/100000: episode: 186, duration: 4.336s, episode steps: 1000, steps per second: 231, episode reward: -117.282, mean reward: -0.117 [-3.962, 3.819], mean action: 1.617 [0.000, 3.000], mean observation: 0.160 [-0.216, 1.419], loss: 4.436273, mean_absolute_error: 22.195278, mean_q: 26.050268\n",
      " 98331/100000: episode: 187, duration: 3.167s, episode steps: 802, steps per second: 253, episode reward: -169.517, mean reward: -0.211 [-100.000, 4.285], mean action: 1.491 [0.000, 3.000], mean observation: 0.211 [-0.182, 1.410], loss: 4.188486, mean_absolute_error: 22.095638, mean_q: 25.728851\n",
      " 98838/100000: episode: 188, duration: 2.061s, episode steps: 507, steps per second: 246, episode reward: -149.043, mean reward: -0.294 [-100.000, 3.640], mean action: 1.655 [0.000, 3.000], mean observation: 0.216 [-0.243, 1.399], loss: 5.139307, mean_absolute_error: 22.069666, mean_q: 25.639673\n",
      " 99718/100000: episode: 189, duration: 4.185s, episode steps: 880, steps per second: 210, episode reward: -160.739, mean reward: -0.183 [-100.000, 4.379], mean action: 1.644 [0.000, 3.000], mean observation: 0.229 [-0.221, 1.518], loss: 4.519883, mean_absolute_error: 22.092052, mean_q: 25.653374\n",
      " 99966/100000: episode: 190, duration: 1.015s, episode steps: 248, steps per second: 244, episode reward: -128.702, mean reward: -0.519 [-100.000, 4.431], mean action: 1.831 [0.000, 3.000], mean observation: 0.185 [-0.719, 1.386], loss: 6.140026, mean_absolute_error: 21.593620, mean_q: 25.301577\n",
      "done, took 435.593 seconds\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average total reward for 200 games: -161.32143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Results, Comparison and Computation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "The aircraft generally lands upside down, which suggest overshooting or persistence in the action choice when going left or right. The aircraft also tends to just fall down to the platform. The image dataset was unbalanced dataset which have more data for the classes 0 and 2. Therefore, in order to balance the dataset we have downsized these two classes to similar number of images with other two classes. We have trained two models in this section with different parameters. Each of them have performed poorly and created similar results in the game. The average total rewards were -336.5 and 377.2, respectively.\n",
    "\n",
    "Training for the convolutional neural network is extremely slow compared to reinforcement learning model with each epoch taking on average 1500 seconds (approximately 25 minutes) with batch size equal to 32. After 4 epochs, these models achieve approx 65% training accuracies and 68-69% validation accuracies. These results say that it is significantly more computationally expensive to use image data given its high level of dimensionality (84x84x1), even after downsizing and conversion to grayscale, to get comparable accuracy to a supervised model with the state features.\n",
    "\n",
    "### Reinforcement Learning\n",
    "Reinforcement learning using Deep-Q Learning and the state data performs well in general and improves upon the results from the convolutional neural network. In general, reinforcement learning plays a more sophisticated game using a combination of actions leading to an upright landing, a landing point closer to the target range, and slower landing based on velocity. \n",
    "\n",
    "Additionally, We tested the results between training the model using epsilon greedy policy. We trained this model for 100,000 steps, which took 4-5 minutes approximately. The model trained with epsilon greedy policy performs better than concolutional neural network model, yielding an average total reward of -130.8.\n",
    "\n",
    "Finally, we would consider Deep Q-Learning with epsilon greedy policy to be the best performing model. Since, it takes less time for training and testing and also with higher reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
